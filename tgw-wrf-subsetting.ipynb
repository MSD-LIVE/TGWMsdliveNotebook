{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56afa-bc63-4f1a-8502-a6f38e1d3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs # just for plotting\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt # just for plotting\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pygris import counties, states\n",
    "import xarray as xr # also need to install netcdf4 and dask[complete]\n",
    "import zarr\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94854ed5-c16c-4c0b-b519-da539e7647c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ceec38-5227-408d-97c0-80f28ead0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to fix the weird WRF indexing in the original NetCDF files\n",
    "# and load the time/space dimensions into memory\n",
    "#\n",
    "# def preprocess(d):\n",
    "#     d = d.rename_dims({\n",
    "#         'Time': 'time',\n",
    "#     }).rename_vars({\n",
    "#         'XLAT': 'lat',\n",
    "#         'XLONG': 'lon',\n",
    "#     })\n",
    "#     d['time'] = pd.to_datetime(\n",
    "#         d.Times.load().astype(str).str.replace('_', ' ')\n",
    "#     )\n",
    "#     d = d.drop_vars(['Times'])\n",
    "#     d['lat'] = d.lat.isel(time=0).load()\n",
    "#     d['lon'] = d.lon.isel(time=0).load()\n",
    "#     return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d3ebc-8b1b-406c-9dac-a2e3630780f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this to convert the NetCDFs\n",
    "# NOTE that it may be important to process them in time order...\n",
    "# NOTE not sure what happens if you append_dim out of order\n",
    "#\n",
    "# for i, f in enumerate(sorted(glob('./tgw_wrf_*.nc'))):\n",
    "#     d = xr.open_mfdataset(f, preprocess=preprocess)\n",
    "#     if i==0:\n",
    "#         d.to_zarr('./tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "#     else:\n",
    "#         d.to_zarr('./tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr', append_dim='time')\n",
    "#     d.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bc934-a73d-4f37-a2fc-c1ee7bc4d0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad7bdc-6f1b-44c6-b9a7-fd74e4c4646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset of counties and states for subsetting\n",
    "\n",
    "# CONUS states\n",
    "conus_states = states(cb=True, year=2020, cache=True).to_crs(\"epsg:4326\")\n",
    "conus_states = conus_states[~conus_states.NAME.isin([\n",
    "    'Alaska','American Samoa','Puerto Rico','United States Virgin Islands',\n",
    "    'Hawaii','Guam','Commonwealth of the Northern Mariana Islands',\n",
    "])]\n",
    "\n",
    "# CONUS counties\n",
    "conus_counties = counties(cb=True, year=2020, cache=True).to_crs(\"epsg:4326\")\n",
    "conus_counties = conus_counties[conus_counties.STATEFP.isin(\n",
    "    conus_states.STATEFP\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc7ad7-b119-460c-94ae-313130e2bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tgw_subset(\n",
    "    *,\n",
    "    start: str, # 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    end: str, # 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    county_fips: str = None, # county FIPS code to keep, None for all\n",
    "    state_abbreviation: str = None, # State abbreviation to keep, None for all\n",
    "    min_lat: float = None, # minimum latitude in WGS84 (epsg:4326)\n",
    "    max_lat: float = None, # maximum latitude in WGS84 (epsg:4326)\n",
    "    min_lon: float = None, # minimum longitude in WGS84 (epsg:4326)\n",
    "    max_lon: float = None, # maximum longitude in WGS84 (epsg:4326)\n",
    "    variables = None, # list of variables to keep, None for all\n",
    "    data_store = './data/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr', # path to the zarr\n",
    "    load = True, # if True, load the data before returning; otherwise return the chunked dask dataset\n",
    "    write_to_file = False, # if a path, write subset to that path; if False don't\n",
    "):\n",
    "\n",
    "    # NOTE that certain variables (precipitation, etc) are presented as \"cumulative\",\n",
    "    #      meaning that the user may actually need one timestep before the requested\n",
    "    #      start time in order to fully resolve those variables\n",
    "    # TODO this is not accounted for in this method\n",
    "\n",
    "    # NOTE that the WRF data presented in WGS84 (epsg:4326) projection as is the case\n",
    "    #      here is NOT on a rectilinear grid, which can be confusing to work with, but\n",
    "    #      the native WRF projection IS on a rectilinear grid but those coordinates are\n",
    "    #      not provided by default (see the python package salem for more details...)\n",
    "\n",
    "    # NOTE the data_store must be used to filter by scenario,\n",
    "    #      but users may benefit from a wrapper for that functionality too\n",
    "\n",
    "    # open the files with dask chunks\n",
    "    # TODO may be more efficient chunking method than the default...\n",
    "    # d = xr.open_mfdataset(data_store, engine='zarr', parallel=True)\n",
    "    \n",
    "    import s3fs\n",
    "    import configparser\n",
    "    import os\n",
    "    # Load AWS credentials from the file\n",
    "    # config = configparser.ConfigParser()\n",
    "    # config.read(os.getenv('AWS_SHARED_CREDENTIALS_FILE'))\n",
    "    # aws_access_key_id = config.get('default', 'aws_access_key_id')\n",
    "    # aws_secret_access_key = config.get('default', 'aws_secret_access_key')\n",
    "    \n",
    "    # print(f'aws_access_key_id {aws_access_key_id}')\n",
    "    # print(f'aws_secret_access_key {aws_secret_access_key}')\n",
    "    \n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     key=aws_access_key_id,\n",
    "    #     secret=aws_secret_access_key,\n",
    "    #     client_kwargs={\n",
    "    #     'endpoint_url': 'https://8mg1a-s4774-889772541283.s3-accesspoint.us-west-2.amazonaws.com'\n",
    "    #     }\n",
    "    # )\n",
    "    # mapper = s3.get_mapper('8mg1a-s4774/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "    # d = xr.open_mfdataset(mapper, engine='zarr', parallel=True)\n",
    "\n",
    "\n",
    "    # worked with zoe's admin creds copied to ec2 instance's aws_creds file (sudo dnf install nano) \n",
    "    # stop service that refreshes creds once a min was: \n",
    "    # sudo systemctl stop msdlive_creds.service \n",
    "    # sudo systemctl stop msdlive_creds.timer\n",
    "    \n",
    "    # but crashed the kernel when doing a 1 month run\n",
    "    # s3 = s3fs.S3FileSystem()\n",
    "    # mapper = s3.get_mapper('msdlive-project-cats-dev/8mg1a-s4774/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "    # d = xr.open_zarr(mapper, parallel=True)\n",
    "    \n",
    "    # did not work:\n",
    "    # d = xr.open_mfdataset(mapper, engine='zarr', parallel=True)\n",
    "    \n",
    "    # Create an S3FileSystem object\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    \n",
    "    # Use the Access Point ARN\n",
    "    # Replace 'region', 'account-id', and 'access-point-name' with your actual values\n",
    "    access_point_arn = 'arn:aws:s3:us-west-2:889772541283:accesspoint/8mg1a-s4774'\n",
    "    \n",
    "    # Now, use this ARN to get the mapper\n",
    "    # Append your specific path after the ARN\n",
    "    mapper = s3.get_mapper(f'{access_point_arn}/8mg1a-s4774/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "    d = xr.open_zarr(mapper, consolidated=True)\n",
    "    \n",
    "\n",
    "    # load the coordinates so they can be used as indexers\n",
    "    d.lat.load();\n",
    "    d.lon.load();\n",
    "    d.time.load();\n",
    "\n",
    "    # subset by variables\n",
    "    # TODO may help to ignore or just warn about requested variables that don't\n",
    "    #      exist, rather than just fail\n",
    "    if variables is not None:\n",
    "        d = d[variables]\n",
    "    \n",
    "    # subset by date\n",
    "    d = d.sel(time=slice(start, end))\n",
    "\n",
    "    # subset by space\n",
    "    # NOTE that there could be errors caused by use of -180 to 180 vs 0 to 360 nomenclature\n",
    "    # TODO may want to build in a buffer to be sure to catch the edges of the shape\n",
    "    if (state_abbreviation is not None):\n",
    "        state_bounds = conus_states[conus_states.STUSPS == state_abbreviation.upper()].bounds.iloc[0]\n",
    "        d = d.where(\n",
    "            (d.lat>=state_bounds.miny) &\n",
    "            (d.lat<=state_bounds.maxy) &\n",
    "            (d.lon>=state_bounds.minx) &\n",
    "            (d.lon<=state_bounds.maxx),\n",
    "            drop=True,\n",
    "        )\n",
    "    if (county_fips is not None):\n",
    "        county_bounds = conus_counties[conus_counties.GEOID == county_fips].bounds.iloc[0]\n",
    "        d = d.where(\n",
    "            (d.lat>=county_bounds.miny) &\n",
    "            (d.lat<=county_bounds.maxy) &\n",
    "            (d.lon>=county_bounds.minx) &\n",
    "            (d.lon<=county_bounds.maxx),\n",
    "            drop=True,\n",
    "        )\n",
    "    if (min_lat is not None) or (max_lat is not None) or (min_lon is not None) or (max_lon is not None):\n",
    "        d = d.where(\n",
    "            (d.lat>=(min_lat if min_lat is not None else -np.Inf)) &\n",
    "            (d.lat<=(max_lat if max_lat is not None else np.Inf)) &\n",
    "            (d.lon>=(min_lon if min_lon is not None else -np.Inf)) &\n",
    "            (d.lon<=(max_lon if max_lon is not None else np.Inf)),\n",
    "            drop=True,\n",
    "        )\n",
    "\n",
    "    # write the data to file if requested\n",
    "    if write_to_file:\n",
    "        d.to_netcdf(write_to_file)\n",
    "\n",
    "    # loading the data fully into memory takes some time\n",
    "    # a user skilled with dask may benefit from keeping the data unloaded\n",
    "    # until the end of their data transformations\n",
    "    if load:\n",
    "        return d.load()\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a41682-234e-4c50-b13d-44dcfc7dcd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ea810-8b50-4d76-9976-6e641fa45317",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "HOME = os.environ.get(\"HOME\")\n",
    "DATA_DIR = os.path.join(HOME, \"data/s3\")\n",
    "d = get_tgw_subset(\n",
    "    start='2088-01-01',\n",
    "    end='2088-02-01T00:00:00',\n",
    "    variables=['T2'],\n",
    "    # county_fips='53033',\n",
    "    state_abbreviation='wa',\n",
    "    # min_lat=45.543830,\n",
    "    # max_lat=49.002405,\n",
    "    # min_lon=-124.7336,\n",
    "    # max_lon=-116.9161,\n",
    "    data_store=f'{DATA_DIR}/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr',\n",
    "    load=False,\n",
    "    # write_to_file='./subset.nc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ee56c-13c4-41d6-af00-ea0426800907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the subset\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc66f2-5177-4b02-bea0-4c19aeb2c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# plot the subset in the usual WGS84 datum\n",
    "fig = plt.figure(figsize=(10.8, 7.2), dpi=150, layout='tight')\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(), frameon=False)\n",
    "conus_counties[conus_counties.STUSPS == 'WA'].boundary.plot(ax=ax, transform=ccrs.PlateCarree(), linewidth=0.5, color='black')\n",
    "d.isel(time=0).T2.plot(ax=ax, transform=ccrs.PlateCarree(), x=\"lon\", y=\"lat\", alpha=0.5, cmap='coolwarm')\n",
    "ax.set_title('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74c228-4341-441b-bf22-bd5060eb9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# plot the subset in the TGW-WRF native projection\n",
    "# '+proj=lcc +lat_0=40.0000076293945 +lon_0=-97 +lat_1=30 +lat_2=45 +x_0=0 +y_0=0 +R=6370000 +units=m +no_defs'\n",
    "tgw_crs = ccrs.LambertConformal(\n",
    "    central_longitude=-97.0,\n",
    "    central_latitude=40.0000076293945,\n",
    "    standard_parallels=(30, 45),\n",
    "    globe=None,\n",
    ")\n",
    "fig = plt.figure(figsize=(10.8, 7.2), dpi=150, layout='tight')\n",
    "ax = plt.axes(projection=tgw_crs, frameon=False)\n",
    "conus_counties[conus_counties.STUSPS == 'WA'].boundary.plot(ax=ax, transform=ccrs.PlateCarree(), linewidth=0.5, color='black')\n",
    "d.isel(time=0).T2.plot(ax=ax, transform=ccrs.PlateCarree(), x=\"lon\", y=\"lat\", alpha=0.5, cmap='coolwarm')\n",
    "ax.set_title('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765748be-b740-4199-89f4-61ae91bd062c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc361145-2541-4d00-814d-8fd7c7645267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godeeep",
   "language": "python",
   "name": "godeeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
