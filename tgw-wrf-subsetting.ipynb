{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56afa-bc63-4f1a-8502-a6f38e1d3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs # just for plotting\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt # just for plotting\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pygris import counties, states\n",
    "import xarray as xr # also need to install netcdf4 and dask[complete]\n",
    "import zarr\n",
    "import fsspec\n",
    "from datetime import datetime, timedelta\n",
    "from dask import delayed\n",
    "import dask\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94854ed5-c16c-4c0b-b519-da539e7647c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ceec38-5227-408d-97c0-80f28ead0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to fix the weird WRF indexing in the original NetCDF files\n",
    "# and load the time/space dimensions into memory\n",
    "#\n",
    "# def preprocess(d):\n",
    "#     d = d.rename_dims({\n",
    "#         'Time': 'time',\n",
    "#     }).rename_vars({\n",
    "#         'XLAT': 'lat',\n",
    "#         'XLONG': 'lon',\n",
    "#     })\n",
    "#     d['time'] = pd.to_datetime(\n",
    "#         d.Times.load().astype(str).str.replace('_', ' ')\n",
    "#     )\n",
    "#     d = d.drop_vars(['Times'])\n",
    "#     d['lat'] = d.lat.isel(time=0).load()\n",
    "#     d['lon'] = d.lon.isel(time=0).load()\n",
    "#     return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d3ebc-8b1b-406c-9dac-a2e3630780f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this to convert the NetCDFs\n",
    "# NOTE that it may be important to process them in time order...\n",
    "# NOTE not sure what happens if you append_dim out of order\n",
    "#\n",
    "# for i, f in enumerate(sorted(glob('./tgw_wrf_*.nc'))):\n",
    "#     d = xr.open_mfdataset(f, preprocess=preprocess)\n",
    "#     if i==0:\n",
    "#         d.to_zarr('./tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "#     else:\n",
    "#         d.to_zarr('./tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr', append_dim='time')\n",
    "#     d.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bc934-a73d-4f37-a2fc-c1ee7bc4d0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad7bdc-6f1b-44c6-b9a7-fd74e4c4646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset of counties and states for subsetting\n",
    "\n",
    "# CONUS states\n",
    "conus_states = states(cb=True, year=2020, cache=True).to_crs(\"epsg:4326\")\n",
    "conus_states = conus_states[~conus_states.NAME.isin([\n",
    "    'Alaska','American Samoa','Puerto Rico','United States Virgin Islands',\n",
    "    'Hawaii','Guam','Commonwealth of the Northern Mariana Islands',\n",
    "])]\n",
    "\n",
    "# CONUS counties\n",
    "conus_counties = counties(cb=True, year=2020, cache=True).to_crs(\"epsg:4326\")\n",
    "conus_counties = conus_counties[conus_counties.STATEFP.isin(\n",
    "    conus_states.STATEFP\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(start: str):\n",
    "    # Check if the string contains time\n",
    "    if 'T' in start:\n",
    "        format_str = '%Y-%m-%dT%H:%M:%S'  # Format for datetime with time\n",
    "    else:\n",
    "        format_str = '%Y-%m-%d'  # Format for datetime without time\n",
    "\n",
    "    start_date = datetime.strptime(start, format_str)\n",
    "    return start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc7ad7-b119-460c-94ae-313130e2bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tgw_subset(\n",
    "    *,\n",
    "    start: str, # 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    end: str, # 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    county_fips: str = None, # county FIPS code to keep, None for all\n",
    "    state_abbreviation: str = None, # State abbreviation to keep, None for all\n",
    "    min_lat: float = None, # minimum latitude in WGS84 (epsg:4326)\n",
    "    max_lat: float = None, # maximum latitude in WGS84 (epsg:4326)\n",
    "    min_lon: float = None, # minimum longitude in WGS84 (epsg:4326)\n",
    "    max_lon: float = None, # maximum longitude in WGS84 (epsg:4326)\n",
    "    variables = None, # list of variables to keep, None for all\n",
    "    data_store = './data/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr', # path to the zarr\n",
    "    load = True, # if True, load the data before returning; otherwise return the chunked dask dataset\n",
    "    write_to_file = False, # if a path, write subset to that path; if False don't\n",
    "):\n",
    "\n",
    "    # NOTE that certain variables (precipitation, etc) are presented as \"cumulative\",\n",
    "    #      meaning that the user may actually need one timestep before the requested\n",
    "    #      start time in order to fully resolve those variables\n",
    "    # TODO this is not accounted for in this method\n",
    "\n",
    "    # NOTE that the WRF data presented in WGS84 (epsg:4326) projection as is the case\n",
    "    #      here is NOT on a rectilinear grid, which can be confusing to work with, but\n",
    "    #      the native WRF projection IS on a rectilinear grid but those coordinates are\n",
    "    #      not provided by default (see the python package salem for more details...)\n",
    "\n",
    "    # NOTE the data_store must be used to filter by scenario,\n",
    "    #      but users may benefit from a wrapper for that functionality too\n",
    "\n",
    "    # open the files with dask chunks\n",
    "    # TODO may be more efficient chunking method than the default...\n",
    "    # d = xr.open_mfdataset(data_store, engine='zarr', parallel=True)\n",
    "\n",
    "    # Create an S3FileSystem object\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    \n",
    "    # Use the Access Point ARN\n",
    "    # Replace 'region', 'account-id', and 'access-point-name' with your actual values\n",
    "    access_point_arn = 'arn:aws:s3:us-west-2:889772541283:accesspoint/8mg1a-s4774'\n",
    "    \n",
    "    # Now, use this ARN to get the mapper\n",
    "    # Append your specific path after the ARN\n",
    "    mapper = s3.get_mapper(f'{access_point_arn}/8mg1a-s4774/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "    d = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "    @dask.delayed\n",
    "    def process_and_save_data(d, day_start, day_end):\n",
    "        # load the coordinates so they can be used as indexers\n",
    "        d.lat.load();\n",
    "        d.lon.load();\n",
    "        d.time.load();\n",
    "    \n",
    "        # subset by variables\n",
    "        # TODO may help to ignore or just warn about requested variables that don't\n",
    "        #      exist, rather than just fail\n",
    "        if variables is not None:\n",
    "            d = d[variables]\n",
    "            \n",
    "        # subset by date\n",
    "        d = d.sel(time=slice(day_start, day_end))\n",
    "    \n",
    "        # subset by space\n",
    "        # NOTE that there could be errors caused by use of -180 to 180 vs 0 to 360 nomenclature\n",
    "        # TODO may want to build in a buffer to be sure to catch the edges of the shape\n",
    "        if (state_abbreviation is not None):\n",
    "            state_bounds = conus_states[conus_states.STUSPS == state_abbreviation.upper()].bounds.iloc[0]\n",
    "            d = d.where(\n",
    "                (d.lat>=state_bounds.miny) &\n",
    "                (d.lat<=state_bounds.maxy) &\n",
    "                (d.lon>=state_bounds.minx) &\n",
    "                (d.lon<=state_bounds.maxx),\n",
    "                drop=True,\n",
    "            )\n",
    "        if (county_fips is not None):\n",
    "            county_bounds = conus_counties[conus_counties.GEOID == county_fips].bounds.iloc[0]\n",
    "            d = d.where(\n",
    "                (d.lat>=county_bounds.miny) &\n",
    "                (d.lat<=county_bounds.maxy) &\n",
    "                (d.lon>=county_bounds.minx) &\n",
    "                (d.lon<=county_bounds.maxx),\n",
    "                drop=True,\n",
    "            )\n",
    "        if (min_lat is not None) or (max_lat is not None) or (min_lon is not None) or (max_lon is not None):\n",
    "            d = d.where(\n",
    "                (d.lat>=(min_lat if min_lat is not None else -np.Inf)) &\n",
    "                (d.lat<=(max_lat if max_lat is not None else np.Inf)) &\n",
    "                (d.lon>=(min_lon if min_lon is not None else -np.Inf)) &\n",
    "                (d.lon<=(max_lon if max_lon is not None else np.Inf)),\n",
    "                drop=True,\n",
    "            )\n",
    "    \n",
    "        # write the data to file if requested\n",
    "        if write_to_file:\n",
    "            # Generate the file name for the current day\n",
    "            file_name = f\"./subselected_data_{day_start.strftime('%Y-%m-%d')}.nc\"\n",
    "            d.load().to_netcdf(file_name)\n",
    "\n",
    "    # loading the data fully into memory takes some time\n",
    "    # a user skilled with dask may benefit from keeping the data unloaded\n",
    "    # until the end of their data transformations\n",
    "    # if load:\n",
    "    #     return d.load()\n",
    "\n",
    "    # List to store delayed tasks\n",
    "    tasks = []\n",
    "\n",
    "    start_date = convert_to_datetime(start)\n",
    "    end_date = convert_to_datetime(end)\n",
    "\n",
    "    # Loop through each day of January\n",
    "    while start_date <= end_date:\n",
    "        # Define the start and end dates for the current day\n",
    "        day_start = start_date\n",
    "        day_end = start_date + timedelta(days=1)\n",
    "    \n",
    "        # Append delayed task to the list\n",
    "        tasks.append(process_and_save_data(day_start, day_end))\n",
    "    \n",
    "        # Move to the next day\n",
    "        start_date += timedelta(days=1)\n",
    "    \n",
    "    # Execute delayed tasks in parallel\n",
    "    dask.compute(*tasks)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ea810-8b50-4d76-9976-6e641fa45317",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "HOME = os.environ.get(\"HOME\")\n",
    "DATA_DIR = os.path.join(HOME, \"data/s3\")\n",
    "d = get_tgw_subset(\n",
    "    start='2088-01-01',\n",
    "    end='2088-02-01T00:00:00',\n",
    "    variables=['T2'],\n",
    "    # county_fips='53033',\n",
    "    state_abbreviation='wa',\n",
    "    # min_lat=45.543830,\n",
    "    # max_lat=49.002405,\n",
    "    # min_lon=-124.7336,\n",
    "    # max_lon=-116.9161,\n",
    "    data_store=f'{DATA_DIR}/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr',\n",
    "    load=False,\n",
    "    # write_to_file='./subset.nc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ced62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3FileSystem object\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "# Use the Access Point ARN\n",
    "# Replace 'region', 'account-id', and 'access-point-name' with your actual values\n",
    "access_point_arn = 'arn:aws:s3:us-west-2:889772541283:accesspoint/8mg1a-s4774'\n",
    "\n",
    "# Now, use this ARN to get the mapper\n",
    "# Append your specific path after the ARN\n",
    "mapper = s3.get_mapper(f'{access_point_arn}/8mg1a-s4774/tgw_wrf_rcp85hotter_hourly_2088_default_chunks.zarr')\n",
    "ds = xr.open_zarr(mapper, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40849e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from dask import delayed\n",
    "import dask\n",
    "\n",
    "start_date = datetime(2088, 1, 1)\n",
    "end_date = datetime(2088, 2, 1)\n",
    "variables=['T2']\n",
    "state_abbreviation='wa'\n",
    "load=False\n",
    "\n",
    "\n",
    "# Loop through each day of January\n",
    "d.lat.load()\n",
    "d.lon.load()\n",
    "d.time.load()\n",
    "\n",
    "@dask.delayed\n",
    "def process_and_save_data(day_start, day_end):\n",
    "    # subset by date\n",
    "    selected_data = ds[variables].sel(time=slice(day_start, day_end))\n",
    "\n",
    "    # subset by space\n",
    "    # NOTE that there could be errors caused by use of -180 to 180 vs 0 to 360 nomenclature\n",
    "    # TODO may want to build in a buffer to be sure to catch the edges of the shape\n",
    "    if (state_abbreviation is not None):\n",
    "        state_bounds = conus_states[conus_states.STUSPS == state_abbreviation.upper()].bounds.iloc[0]\n",
    "        selected_data = selected_data.where(\n",
    "            (d.lat>=state_bounds.miny) &\n",
    "            (d.lat<=state_bounds.maxy) &\n",
    "            (d.lon>=state_bounds.minx) &\n",
    "            (d.lon<=state_bounds.maxx),\n",
    "            drop=True,\n",
    "        )\n",
    "\n",
    "    # # Load the data into memory\n",
    "    # selected_data = selected_data.compute()\n",
    "\n",
    "    # # # Create a new dataset with the extracted data\n",
    "    # new_ds = xr.Dataset({'T2': selected_data}, attrs=d.attrs)\n",
    "\n",
    "    # Generate the file name for the current day\n",
    "    file_name = f\"./selected_data_{day_start.strftime('%Y-%m-%d')}.nc\"\n",
    "\n",
    "    # Save the new dataset to a netCDF file\n",
    "    selected_data.to_netcdf(file_name, engine='netcdf4')\n",
    "\n",
    "# List to store delayed tasks\n",
    "tasks = []\n",
    "\n",
    "# Loop through each day of January\n",
    "while start_date <= end_date:\n",
    "    # Define the start and end dates for the current day\n",
    "    day_start = start_date\n",
    "    day_end = start_date + timedelta(days=1)\n",
    "\n",
    "    # Append delayed task to the list\n",
    "    tasks.append(process_and_save_data(day_start, day_end))\n",
    "\n",
    "    # Move to the next day\n",
    "    start_date += timedelta(days=1)\n",
    "\n",
    "# Execute delayed tasks in parallel\n",
    "dask.compute(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ee56c-13c4-41d6-af00-ea0426800907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the subset\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc66f2-5177-4b02-bea0-4c19aeb2c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# plot the subset in the usual WGS84 datum\n",
    "fig = plt.figure(figsize=(10.8, 7.2), dpi=150, layout='tight')\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(), frameon=False)\n",
    "conus_counties[conus_counties.STUSPS == 'WA'].boundary.plot(ax=ax, transform=ccrs.PlateCarree(), linewidth=0.5, color='black')\n",
    "d.isel(time=0).T2.plot(ax=ax, transform=ccrs.PlateCarree(), x=\"lon\", y=\"lat\", alpha=0.5, cmap='coolwarm')\n",
    "ax.set_title('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74c228-4341-441b-bf22-bd5060eb9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# plot the subset in the TGW-WRF native projection\n",
    "# '+proj=lcc +lat_0=40.0000076293945 +lon_0=-97 +lat_1=30 +lat_2=45 +x_0=0 +y_0=0 +R=6370000 +units=m +no_defs'\n",
    "tgw_crs = ccrs.LambertConformal(\n",
    "    central_longitude=-97.0,\n",
    "    central_latitude=40.0000076293945,\n",
    "    standard_parallels=(30, 45),\n",
    "    globe=None,\n",
    ")\n",
    "fig = plt.figure(figsize=(10.8, 7.2), dpi=150, layout='tight')\n",
    "ax = plt.axes(projection=tgw_crs, frameon=False)\n",
    "conus_counties[conus_counties.STUSPS == 'WA'].boundary.plot(ax=ax, transform=ccrs.PlateCarree(), linewidth=0.5, color='black')\n",
    "d.isel(time=0).T2.plot(ax=ax, transform=ccrs.PlateCarree(), x=\"lon\", y=\"lat\", alpha=0.5, cmap='coolwarm')\n",
    "ax.set_title('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765748be-b740-4199-89f4-61ae91bd062c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc361145-2541-4d00-814d-8fd7c7645267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godeeep",
   "language": "python",
   "name": "godeeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
